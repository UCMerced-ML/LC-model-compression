{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lc\n",
    "from lc.torch import ParameterTorch as Param, AsVector, AsIs\n",
    "from lc.compression_types import ConstraintL0Pruning, LowRank, RankSelection, AdaptiveQuantization\n",
    "from lc.models.torch import lenet300_classic, lenet300_modern_drop, lenet300_modern\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc_loss(forward_func, data_loader):\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    for batch_idx, (x, target) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            target = target.cuda()\n",
    "            score, loss = forward_func(x.cuda(), target)\n",
    "            _, pred_label = torch.max(score.data, 1)\n",
    "            correct_cnt += (pred_label == target.data).sum().item()\n",
    "            ave_loss += loss.data.item() * len(x)\n",
    "    accuracy = correct_cnt * 1.0 / len(data_loader.dataset)\n",
    "    ave_loss /= len(data_loader.dataset)\n",
    "    return accuracy, ave_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We use the MNIST dataset for this demo. The dataset containssubtracted 28x28 grayscale images with digits from 0 to 9. The images are normalized to have grayscale value 0 to 1 and then mean is subtracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "def show_MNIST_images():\n",
    "    train_data_th = datasets.MNIST(root='./datasets', download=True, train=True)\n",
    "    data_train = np.array(train_data_th.data[:])\n",
    "    targets = np.array(train_data_th.targets)\n",
    "    images_to_show = 5\n",
    "    random_indexes = np.random.randint(data_train.shape[0], size=images_to_show)\n",
    "    for i,ind in enumerate(random_indexes):\n",
    "        plt.subplot(1,images_to_show,i+1)\n",
    "        plt.imshow(data_train[ind], cmap='gray')\n",
    "        plt.xlabel(targets[ind])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "show_MNIST_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(batch_size=2048, n_workers=4):\n",
    "    train_data_th = datasets.MNIST(root='./datasets', download=True, train=True)\n",
    "    test_data_th = datasets.MNIST(root='./datasets', download=True, train=False)\n",
    "\n",
    "    data_train = np.array(train_data_th.data[:]).reshape([-1, 28 * 28]).astype(np.float32)\n",
    "    data_test = np.array(test_data_th.data[:]).reshape([-1, 28 * 28]).astype(np.float32)\n",
    "    data_train = (data_train / 255)\n",
    "    dtrain_mean = data_train.mean(axis=0)\n",
    "    data_train -= dtrain_mean\n",
    "    data_test = (data_test / 255).astype(np.float32)\n",
    "    data_test -= dtrain_mean\n",
    "\n",
    "    train_data = TensorDataset(torch.from_numpy(data_train), train_data_th.targets)\n",
    "    test_data = TensorDataset(torch.from_numpy(data_test), test_data_th.targets)\n",
    "\n",
    "    train_loader = DataLoader(train_data, num_workers=n_workers, batch_size=batch_size, shuffle=True,)\n",
    "    test_loader = DataLoader(test_data, num_workers=n_workers, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Network\n",
    "We use cuda capable GPU for our experiments. The network has 3 fully-connected layers with dimensions 784x300, 300x100, and 100x10, and the total of 266200 parameters (which includes biases). The network was trained to have a test error of 1.79%, which is pretty decent result but not as low as you can get with convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_acc_eval_f(net):\n",
    "    train_loader, test_loader = data_loader()\n",
    "    def forward_func(x, target):\n",
    "        y = net(x)\n",
    "        return y, net.loss(y, target)\n",
    "    acc_train, loss_train = compute_acc_loss(forward_func, train_loader)\n",
    "    acc_test, loss_test = compute_acc_loss(forward_func, test_loader)\n",
    "\n",
    "    print(f\"Train err: {100-acc_train*100:.2f}%, train loss: {loss_train}\")\n",
    "    print(f\"TEST ERR: {100-acc_test*100:.2f}%, test loss: {loss_test}\")\n",
    "    \n",
    "def load_reference_lenet300():\n",
    "    net = lenet300_modern().to(device)\n",
    "    state_dict = torch.utils.model_zoo.load_url('https://ucmerced.box.com/shared/static/766axnc8qq429hiqqyqqo07ek46oqoxq.th')\n",
    "\n",
    "    net.load_state_dict(state_dict)\n",
    "    net.to(device)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the model's train and test errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_acc_eval_f(load_reference_lenet300().eval().to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression using the LC toolkit\n",
    "### Step 1: L step\n",
    "We will use same L step with same hyperparamters for all our compression examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_l_step(model, lc_penalty, step):\n",
    "    train_loader, test_loader = data_loader()\n",
    "    params = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    lr = 0.7*(0.98**step)\n",
    "    optimizer = optim.SGD(params, lr=lr, momentum=0.9, nesterov=True)\n",
    "    print(f'L-step #{step} with lr: {lr:.5f}')\n",
    "    epochs_per_step_ = 7\n",
    "    if step == 0:\n",
    "        epochs_per_step_ = epochs_per_step_ * 2\n",
    "    for epoch in range(epochs_per_step_):\n",
    "        avg_loss = []\n",
    "        for x, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            target = target.to(dtype=torch.long, device=device)\n",
    "            out = model(x)\n",
    "            loss = model.loss(out, target) + lc_penalty()\n",
    "            avg_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"\\tepoch #{epoch} is finished.\")\n",
    "        print(f\"\\t  avg. train loss: {np.mean(avg_loss):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Schedule of mu values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_s = [9e-5 * (1.1 ** n) for n in range(20)]\n",
    "# 20 L-C steps in total\n",
    "# total training epochs is 7 x 20 = 140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression time! Pruning\n",
    "Let us prune all but 5% of the weights in the network (5% = 13310 weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = load_reference_lenet300()\n",
    "\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
    "compression_tasks = {\n",
    "    Param(layers, device): (AsVector, ConstraintL0Pruning(kappa=13310), 'pruning')\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()                              # entry point to the LC algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "Now let us quantize each layer with its own codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = load_reference_lenet300()\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
    "\n",
    "compression_tasks = {\n",
    "    Param(layers[0], device): (AsVector, AdaptiveQuantization(k=2), 'layer0_quant'),\n",
    "    Param(layers[1], device): (AsVector, AdaptiveQuantization(k=2), 'layer1_quant'),\n",
    "    Param(layers[2], device): (AsVector, AdaptiveQuantization(k=2), 'layer2_quant')\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixing pruning, low rank, and quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = load_reference_lenet300()\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
    "\n",
    "compression_tasks = {\n",
    "    Param(layers[0], device): (AsVector, ConstraintL0Pruning(kappa=5000), 'pruning'),\n",
    "    Param(layers[1], device): (AsIs, LowRank(target_rank=9, conv_scheme=None), 'low-rank'),\n",
    "    Param(layers[2], device): (AsVector, AdaptiveQuantization(k=2), 'quant')\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive combination of Quantization and Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = load_reference_lenet300()\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
    "\n",
    "compression_tasks = {\n",
    "    Param(layers, device): [\n",
    "        (AsVector, ConstraintL0Pruning(kappa=2662), 'pruning'),\n",
    "        (AsVector, AdaptiveQuantization(k=2), 'quant')\n",
    "    ]\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-rank compression with automatic rank selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = load_reference_lenet300()\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
    "alpha=1e-9\n",
    "compression_tasks = {\n",
    "    Param(layers[0], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[0], normalize=True), \"layer1_lr\"),\n",
    "    Param(layers[1], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[1], normalize=True), \"layer2_lr\"),\n",
    "    Param(layers[2], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[2], normalize=True), \"layer3_lr\")\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScaledTernaryQuantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lc.compression_types import ScaledTernaryQuantization\n",
    "net = load_reference_lenet300()\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
    "\n",
    "compression_tasks = {\n",
    "    Param(layers[0], device): (AsVector, ScaledTernaryQuantization(), 'layer0_quant'),\n",
    "    Param(layers[1], device): (AsVector, ScaledTernaryQuantization(), 'layer1_quant'),\n",
    "    Param(layers[2], device): (AsVector, ScaledTernaryQuantization(), 'layer2_quant')\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScaledBinaryQuantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lc.compression_types import ScaledBinaryQuantization\n",
    "net = load_reference_lenet300()\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
    "\n",
    "compression_tasks = {\n",
    "    Param(layers[0], device): (AsVector, ScaledBinaryQuantization(), 'layer0_quant'),\n",
    "    Param(layers[1], device): (AsVector, ScaledBinaryQuantization(), 'layer1_quant'),\n",
    "    Param(layers[2], device): (AsVector, ScaledBinaryQuantization(), 'layer2_quant')\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
